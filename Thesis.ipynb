{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Experiment,Workspace, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from operator import concat\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image, ImageFilter\n",
    "#from pycocotools.coco import COCO\n",
    "import pydot\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Activation, Flatten, Dropout, GaussianNoise, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from skimage import io as io\n",
    "from skimage import exposure\n",
    "from skimage.transform import resize, integral_image\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import blob_dog, blob_log, blob_doh, daisy, hog, multiblock_lbp, haar_like_feature\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, jaccard_similarity_score, average_precision_score\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.utils import check_random_state\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import traceback\n",
    "import timeit\n",
    "import urllib.request\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from random import shuffle\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.17\n",
      "Wrote the config file config.json to: C:\\Users\\olivier.claessen\\Desktop\\A.I. thesis\\aml_config\\config.json\n",
      "Workspace configuration succeeded. Skip the workspace creation steps below\n",
      "Found existing cpucluster\n",
      "Found existing gpu cluster\n"
     ]
    }
   ],
   "source": [
    "print(azureml.core.VERSION)\n",
    "'''\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"9bce0414-e6b9-4c79-b146-74018a4b09ac\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"Thesis\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"Multi-label_classification\")\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"eastus2\")\n",
    "'''\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"3e9a2408-19d1-4ca2-bb63-339d47d3baaa\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"Thesis\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"Multi-label_back-up\")\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"eastus2\")\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    # write the details of the workspace to a configuration file to the notebook library\n",
    "    ws.write_config()\n",
    "    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Workspace not accessible. Change your parameters or create a new workspace below\")\n",
    "    \n",
    "cpu_cluster_name = \"cpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print(\"Found existing cpucluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new cpucluster\")\n",
    "    \n",
    "    # Specify the configuration for the new cluster\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\",\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # Create the cluster with the specified name and configuration\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    \n",
    "    # Wait for the cluster to complete, show the output log\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "gpu_cluster_name = \"gpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print(\"Found existing gpu cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new gpucluster\")\n",
    "    \n",
    "    # Specify the configuration for the new cluster\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_NC6\",\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=4)\n",
    "    # Create the cluster with the specified name and configuration\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "\n",
    "    # Wait for the cluster to complete, show the output log\n",
    "    gpu_cluster.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote the config file config.json to: C:\\Users\\olivier.claessen\\Desktop\\A.I. thesis\\aml_config\\config.json\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Create the workspace using the specified parameters\n",
    "ws = Workspace.create(name = workspace_name,\n",
    "                      subscription_id = subscription_id,\n",
    "                      resource_group = resource_group, \n",
    "                      location = workspace_region,\n",
    "                      create_resource_group = True,\n",
    "                      exist_ok = True)\n",
    "ws.get_details()\n",
    "\n",
    "# write the details of the workspace to a configuration file to the notebook library\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'multi-class_classification'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. gpucluster\n"
     ]
    }
   ],
   "source": [
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpucluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\") #\"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureBlob multilabelback2880750212 azureml-blobstore-944dae39-f956-45b0-b88a-38ce6482276b\n"
     ]
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "#ds.upload(src_dir='./data', target_path='data', overwrite=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = './coco-multi-label'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"data/annotations/instances_train2014.json\") as read_file:\\n    instances = json.load(read_file)\\nwith open(\"data/annotations/person_keypoints_train2014.json\") as read_file:\\n    keypoints = json.load(read_file)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/annotations/instances_train2014.json\") as read_file:\n",
    "    train = json.load(read_file)\n",
    "\n",
    "with open(\"data/annotations/instances_val2014.json\") as read_file:\n",
    "    val = json.load(read_file)\n",
    "'''\n",
    "with open(\"data/annotations/instances_train2014.json\") as read_file:\n",
    "    instances = json.load(read_file)\n",
    "with open(\"data/annotations/person_keypoints_train2014.json\") as read_file:\n",
    "    keypoints = json.load(read_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'COCO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-235b489649c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdataType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train2014'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mannFile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/annotations/instances_{}.json'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataDir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcoco\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCOCO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'COCO' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in 2014 train: 82783\n",
      "Number of annotations in 2014 train: 604907\n",
      "Number of annotations/image in 2014 train: 7.307140354903785\n",
      "82081\n",
      "Label density in 2014 train: 0.0913392544362973\n",
      "dict_keys(['info', 'images', 'licenses', 'annotations', 'categories'])\n",
      "\n",
      "\n",
      "1person: 185316\n",
      "2bicycle: 4955\n",
      "3car: 30785\n",
      "4motorcycle: 6021\n",
      "5airplane: 3833\n",
      "6bus: 4327\n",
      "7train: 3159\n",
      "8truck: 7050\n",
      "9boat: 7590\n",
      "10traffic light: 9159\n",
      "11fire hydrant: 1316\n",
      "13stop sign: 1372\n",
      "14parking meter: 833\n",
      "15bench: 6751\n",
      "16bird: 7290\n",
      "17cat: 3301\n",
      "18dog: 3774\n",
      "19horse: 4666\n",
      "20sheep: 6654\n",
      "21cow: 5686\n",
      "22elephant: 3905\n",
      "23bear: 903\n",
      "24zebra: 3685\n",
      "25giraffe: 3596\n",
      "27backpack: 6200\n",
      "28umbrella: 7865\n",
      "31handbag: 8778\n",
      "32tie: 4497\n",
      "33suitcase: 4251\n",
      "34frisbee: 1862\n",
      "35skis: 4698\n",
      "36snowboard: 1960\n",
      "37sports ball: 4392\n",
      "38kite: 6560\n",
      "39baseball bat: 2400\n",
      "40baseball glove: 2689\n",
      "41skateboard: 4012\n",
      "42surfboard: 4161\n",
      "43tennis racket: 3411\n",
      "44bottle: 16983\n",
      "46wine glass: 5618\n",
      "47cup: 14513\n",
      "48fork: 3918\n",
      "49knife: 5536\n",
      "50spoon: 4287\n",
      "51bowl: 10064\n",
      "52banana: 6912\n",
      "53apple: 4308\n",
      "54sandwich: 3089\n",
      "55orange: 4597\n",
      "56broccoli: 4927\n",
      "57carrot: 5539\n",
      "58hot dog: 2023\n",
      "59pizza: 4001\n",
      "60donut: 4977\n",
      "61cake: 4551\n",
      "62chair: 27147\n",
      "63couch: 4113\n",
      "64potted plant: 5918\n",
      "65bed: 2905\n",
      "67dining table: 11167\n",
      "70toilet: 2873\n",
      "72tv: 4036\n",
      "73laptop: 3415\n",
      "74mouse: 1517\n",
      "75remote: 4122\n",
      "76keyboard: 1980\n",
      "77cell phone: 4460\n",
      "78microwave: 1189\n",
      "79oven: 2302\n",
      "80toaster: 156\n",
      "81sink: 3933\n",
      "82refrigerator: 1875\n",
      "84book: 17315\n",
      "85clock: 4328\n",
      "86vase: 4623\n",
      "87scissors: 1073\n",
      "88teddy bear: 3442\n",
      "89hair drier: 135\n",
      "90toothbrush: 1377\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 least used features \n",
      "\n",
      "[('hair drier', 135), ('toaster', 156), ('parking meter', 833), ('bear', 903), ('scissors', 1073), ('microwave', 1189), ('fire hydrant', 1316), ('stop sign', 1372), ('toothbrush', 1377)]\n",
      "\n",
      "\n",
      "\n",
      "10 most used features \n",
      "\n",
      "[('handbag', 8778), ('traffic light', 9159), ('bowl', 10064), ('dining table', 11167), ('cup', 14513), ('bottle', 16983), ('book', 17315), ('chair', 27147), ('car', 30785), ('person', 185316)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = train[\"categories\"]\n",
    "#print(categories)\n",
    "train_images = train[\"images\"]\n",
    "val_images = val[\"images\"]\n",
    "#val_images = val[\"images\"]\n",
    "print(\"Number of images in 2014 train: \"+str(len(train_images)))\n",
    "#print(\"Number of images in 2014 val: \"+str(len(val_images)) +\"\\n\")\n",
    "train_annotations = train[\"annotations\"]\n",
    "val_annotations = val[\"annotations\"]\n",
    "#print(train_annotations[1:20])\n",
    "#val_annotations = val[\"annotations\"]\n",
    "print(\"Number of annotations in 2014 train: \"+str(len(train_annotations)))\n",
    "#print(\"Number of annotations in 2014 val: \"+str(len(val_annotations)) + \"\\n\")\n",
    "print(\"Number of annotations/image in 2014 train: \"+str(len(train_annotations)/len(train_images)))\n",
    "#print(\"Number of annotations/image in 2014 val: \" + str(len(val_annotations)/len(val_images)) + \"\\n\")\n",
    "train_categories = np.zeros(100, dtype=object)\n",
    "#val_categories = np.zeros(100, dtype=object)\n",
    "#itemsets = [[] for i in range(581922)]\n",
    "itemsets = defaultdict(list)\n",
    "for annotation in train_annotations:\n",
    "    train_categories[annotation['category_id']] += 1\n",
    "    itemsets[annotation['image_id']].append(annotation['category_id'])\n",
    "unique_itemsets = []\n",
    "for entry in itemsets:\n",
    "    if(entry not in unique_itemsets):\n",
    "        unique_itemsets.append(entry)\n",
    "print(len(unique_itemsets))\n",
    "#for annotation in val_annotations:\n",
    "#    val_categories[annotation['category_id']] += 1\n",
    "train_categories = train_categories[train_categories != 0]\n",
    "#val_categories = val_categories[val_categories != 0]\n",
    "print(\"Label density in 2014 train: \"+str(len(train_annotations)/len(train_categories)/len(train_images)))\n",
    "#print(\"Label density in 2014 val: \"+str(len(val_annotations)/len(val_categories)/len(val_images)))\n",
    "print(train.keys())\n",
    "print(\"\\n\")\n",
    "#print(images[1:10])\n",
    "#print(\"\\n\")\n",
    "#print(annotations[1:10])\n",
    "#print(\"\\n\")\n",
    "#print(categories)\n",
    "for i in range(0, len(train_categories)):\n",
    "    print(str(categories[i][\"id\"]) + categories[i][\"name\"] + \": \" + str(train_categories[i]))\n",
    "    train_categories[i] = (categories[i][\"name\"], train_categories[i])\n",
    "#for i in range(0, len(val_categories)):\n",
    "    #print(categories[i][\"name\"] + \": \" + str(train_categories[i]))\n",
    "#    val_categories[i] = (categories[i][\"name\"], val_categories[i])\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot([i[1] for i in train_categories])\n",
    "#plt.subplot(212)\n",
    "#plt.plot([i[1] for i in val_categories])\n",
    "plt.show()\n",
    "train_categories = sorted(train_categories, key=lambda tup: tup[1], reverse=False)\n",
    "#val_categories = sorted(val_categories, key=lambda tup: tup[1], reverse=False)\n",
    "print(\"\\n10 least used features \\n\")\n",
    "print(train_categories[0:9])\n",
    "print(\"\\n\")\n",
    "#print(val_categories[0:9])\n",
    "print(\"\\n10 most used features \\n\")\n",
    "print(train_categories[-10:])\n",
    "print(\"\\n\")\n",
    "#print(val_categories[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "22\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "image_id = 274467 #9,25,30,34,36,49\n",
    "def findAnnotations(id, property):\n",
    "    items = []\n",
    "    for annotation in train_annotations: \n",
    "        if annotation[property] == id:\n",
    "            items.append(annotation)\n",
    "    return items\n",
    "items = findAnnotations(image_id, 'image_id')\n",
    "for item in items:\n",
    "    print(item[\"category_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findValueDictionary(dict,key,value, returnkey):\n",
    "    for entry in dict:\n",
    "        if entry[key] == value:\n",
    "            return entry[returnkey]\n",
    "\n",
    "image_id_string = str(image_id).zfill(12)\n",
    "image = np.array(Image.open('data/train2014/COCO_train2014_'+ image_id_string +'.jpg'), dtype=np.uint8)\n",
    "\n",
    "colors = np.random.random((len(categories)+10, 3))\n",
    "i=0\n",
    "bbox_list = []\n",
    "item_list = []fig,ax = plt.subplots(1)\n",
    "ax.imshow(image)\n",
    "for item in items:\n",
    "    label = findValueDictionary(categories, 'id', item['category_id'], 'name')\n",
    "    rect = patches.Rectangle((item['bbox'][0], (item['bbox'][1])), item['bbox'][2], item['bbox'][3],linewidth=1,edgecolor=colors[item['category_id']-1],facecolor='none', label= label)\n",
    "    ax.add_patch(rect)\n",
    "    bbox_list.append(rect)\n",
    "    item_list.append(item['category_id'])\n",
    "#print(items)\n",
    "plt.figure(figsize=(3,4))\n",
    "plt.legend(handles=bbox_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Run\n",
    "import cv2\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import os\n",
    "from PIL import Image, ImageFilter\n",
    "from skimage import io as io\n",
    "from skimage import exposure\n",
    "from skimage.transform import resize, integral_image\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import blob_dog, blob_log, blob_doh, corner_harris, corner_subpix, corner_peaks, daisy, hog\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import time\n",
    "\n",
    "def loadAnnotations(fileLocation):\n",
    "    with open(fileLocation) as read_file:\n",
    "        annotations = json.load(read_file)\n",
    "    return annotations\n",
    "    \n",
    "def getBoundingBox(x,y,w,h,img):\n",
    "    return img[y:y+h,x:x+w]\n",
    "\n",
    "def findAnnotations(id, property, annotations):\n",
    "    items = []\n",
    "    for annotation in annotations: \n",
    "        if annotation[property] == id:\n",
    "            items.append(annotation)\n",
    "    return items\n",
    "\n",
    "def zca_whitening_matrix(X):\n",
    "    # Covariance matrix [column-wise variables]: Sigma = (X-mu)' * (X-mu) / N\n",
    "    sigma = np.cov(X, rowvar=True) # [M x M]\n",
    "    # Singular Value Decomposition. X = U * np.diag(S) * V\n",
    "    U,S,V = np.linalg.svd(sigma)\n",
    "        # U: [M x M] eigenvectors of sigma.\n",
    "        # S: [M x 1] eigenvalues of sigma.\n",
    "        # V: [M x M] transpose of U\n",
    "    # Whitening constant: prevents division by zero\n",
    "    epsilon = 1e-5\n",
    "    # ZCA Whitening matrix: U * Lambda * U'\n",
    "    ZCAMatrix = np.dot(U, np.dot(np.diag(1.0/np.sqrt(S + epsilon)), U.T)) # [M x M]\n",
    "    return ZCAMatrix\n",
    "\n",
    "def preProcessImage(img):\n",
    "    img = np.asarray(img)\n",
    "    img = rgb2gray(img)\n",
    "    #img = cv2.GaussianBlur(img,(5,5),0)\n",
    "    #img = cv2.medianBlur(img,5)\n",
    "    #img = cv2.bilateralFilter(img,9,75,75)\n",
    "    #img = cv2.blur(img,(5,5))\n",
    "    #kernel = np.ones((5,5),np.float32)/25\n",
    "    #img = cv2.filter2D(img,-1,kernel)\n",
    "    return img\n",
    "\n",
    "def noisy(image):\n",
    "    row,col,ch = image.shape\n",
    "    s_vs_p = 0.5\n",
    "    amount = 0.004\n",
    "    out = np.copy(image)\n",
    "    # Salt mode\n",
    "    num_salt = np.ceil(amount * image.size * s_vs_p)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "          for i in image.shape]\n",
    "    out[coords] = 1\n",
    "\n",
    "    # Pepper mode\n",
    "    num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))\n",
    "    coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "          for i in image.shape]\n",
    "    out[coords] = 0\n",
    "    return out\n",
    "\n",
    "def getBoundingBoxesAnnotations(annotations, path):\n",
    "    bounded_images = []\n",
    "    bounded_annotations = []\n",
    "    try:\n",
    "        for annotation in annotations:\n",
    "            image_id = annotation['image_id']\n",
    "            image_id_string = str(image_id).zfill(12)\n",
    "            image = Image.open(path + image_id_string +'.jpg').convert('RGB')\n",
    "            image = preProcessImage(image) \n",
    "            image_resized = resize(getBoundingBox(int(annotation['bbox'][0]),int(annotation['bbox'][1]),int(annotation['bbox'][2]),int(annotation['bbox'][3]),image)\n",
    "                                   , (48, 48),\n",
    "                           anti_aliasing=True)\n",
    "            bounded_images.append(image_resized)\n",
    "            bounded_annotations.append(annotation['category_id'])     \n",
    "    except Exception as ex:\n",
    "            print(ex)   \n",
    "    bounded_images = np.asarray(bounded_images)\n",
    "    bounded_annotations = np.asarray(bounded_annotations)\n",
    "    return bounded_images,bounded_annotations\n",
    "\n",
    "def calculateHogFeatures(gray_image, o, pixels, cells):\n",
    "    features = hog(gray_image, orientations=o, \n",
    "                              pixels_per_cell=(pixels, pixels),\n",
    "                              cells_per_block=(cells, cells), \n",
    "                              transform_sqrt=True, \n",
    "                              visualize=False, block_norm = \"L2-Hys\")\n",
    "    return features\n",
    "def calculateDaisyFeatures(gray_image):\n",
    "    descs = daisy(gray_image, step=180, radius=8, rings=3, histograms=6,\n",
    "                         orientations=8, visualize=False)\n",
    "    descs_num = descs.shape[0] * descs.shape[1]\n",
    "    return descs.reshape(descs.size).tolist()\n",
    "\n",
    "def calculateDoG(gray_image):\n",
    "    blobs_dog = blob_dog(gray_image, max_sigma=30, threshold=.1)\n",
    "    blobs_dog[:, 2] = blobs_dog[:, 2] * sqrt(2)\n",
    "    return blobs_dog\n",
    "\n",
    "def calculateSIFT(sift, gray_image):\n",
    "    kp = sift.detect(gray_image,None)\n",
    "    return kp\n",
    "\n",
    "def calculateFeatures(imgs):\n",
    "    #sift = cv2.xfeatures2d.SIFT_create()\n",
    "    #detector = CENSURE()\n",
    "    hog_features = []\n",
    "    dog_features = []\n",
    "    daisy_features = []\n",
    "    sift_features = []\n",
    "    lbp_features = []\n",
    "    haar_features = []\n",
    "    i = 0;\n",
    "    for img in imgs:\n",
    "        hog_features.append(calculateHogFeatures(img,8,12,1))\n",
    "        daisy_features.append(calculateDaisyFeatures(img))\n",
    "        #int_img = integral_image(img)\n",
    "        #lbp_features.append(multiblock_lbp(int_img, 0, 0, 3, 3))\n",
    "        #haar_features.append(haar_like_feature(int_img, 0, 0, 48, 48, 'type-3-x'))\n",
    "        #shape_index_features.append(shape_index(img))\n",
    "        #dog_features.append(calculateDoG(img))\n",
    "        #harris_features.append(corner_peaks(corner_harris(img), min_distance=5))\n",
    "        #sift_features.append(calculateSIFT(sift, img))\n",
    "    return hog_features, daisy_features, lbp_features, haar_features\n",
    "\n",
    "def svmFit(x_train, y_train):\n",
    "    print(\"Fitting the classifier to train\")\n",
    "    #t0 = time()\n",
    "    '''\n",
    "    param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "                  'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "    clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'),\n",
    "                       param_grid, cv=5)\n",
    "    '''\n",
    "    clf = OneVsRestClassifier(LinearSVC())\n",
    "    #clf = OneVsRestClassifier(SVC(kernel='poly'))\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    #print(\"Completed in %0.3fs\" % (time() - t0))\n",
    "    #print(\"Best estimator found by grid search:\")\n",
    "    #print(clf.best_estimator_)\n",
    "    return clf\n",
    "\n",
    "def saveModel(model, filename):\n",
    "    print(\"Saving file...\")\n",
    "    joblib.dump(model, open(filename, 'wb'))\n",
    "    print(\"File saved\")\n",
    "    \n",
    "def loadModel(filename):\n",
    "    print(\"loading file...\")\n",
    "    joblib.load(filename)\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "def svmPredict(x_test, y_test, model):\n",
    "    print(\"Predicting the test set\")\n",
    "    #t0 = time()\n",
    "    y_pred = model.predict(x_test)\n",
    "    #print(\"Completed in %0.3fs\" % (time() - t0))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "def randomForestFit(x_train, y_train, estimators):\n",
    "    print(\"Fitting the classifier to train\")\n",
    "    t0 = time()\n",
    "    rf = RandomForestClassifier(n_estimators=estimators)\n",
    "    rf.fit(x_train, y_train);\n",
    "    print(\"Completed in %0.3fs\" % (time() - t0))\n",
    "    return rf\n",
    "\n",
    "def rfPredict(x_test, y_test, model):\n",
    "    print(\"Predicting the test set\")\n",
    "    t0 = time()\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"Completed in %0.3fs\" % (time() - t0))\n",
    "    errors = abs(y_pred - y_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def constructCNN(conv_layer, layer_size, kernel_size, dense_size, dense_layer, dropout, num_classes, x_train):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(layer_size, (kernel_size, kernel_size), input_shape=x_train.shape[1:]))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    for l in range(conv_layer-1):\n",
    "        model.add(Conv2D(layer_size, (kernel_size, kernel_size)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    if(dropout):\n",
    "        model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    for j in range(dense_layer):\n",
    "        model.add(Dense(dense_size))\n",
    "        model.add(Activation('relu'))\n",
    "    if(dropout):\n",
    "        model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    \n",
    "# Create a function called \"chunks\" with two arguments, l and n:\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]\n",
    "\n",
    "def splitPreprocessing(splits, data_folder, path):\n",
    "    labels = []\n",
    "    features = np.asarray([[],[]])\n",
    "    for split in splits:\n",
    "        bboxes = getBoundingBoxesAnnotations(split, os.path.join(data_folder, path))\n",
    "        splitfeatures = calculateFeatures(bboxes[0])\n",
    "        features = np.append(features, splitfeatures,1)\n",
    "        labels.extend(bboxes[1])\n",
    "    return labels, features.tolist()\n",
    "\n",
    "def non_max_suppression_slow(boxes, overlapThresh):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    " \n",
    "    no_overlap = []\n",
    " \n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    " \n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "    while len(idxs) > 0:\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        no_overlap.append(i)\n",
    " \n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    " \n",
    "\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    " \n",
    "    return boxes[pick].astype(\"int\")\n",
    "\n",
    "def IoU(box_1,box_2):\n",
    "    x_a = max(box_1[0],box_2[0])\n",
    "    y_a = max(box_1[1],box_2[1])\n",
    "    x_b = min(box_1[2],box_2[2])\n",
    "    y_b = min(box_1[3],box_2[3])\n",
    "    \n",
    "    overlap = max(0, x_b - x_a + 1) * max(0, y_b - y_a + 1)\n",
    "    box_1_area = (box_1[2] - box_1[0] + 1) * (box_1[3] - box_1[1] + 1)\n",
    "    box_2_area = (box_2[2] - box_2[0] + 1) * (box_2[3] - box_2[1] + 1)\n",
    "    IoU = overlap/float(box_1_area + box_2_area + overlap)\n",
    "    \n",
    "    return IoU\n",
    "\n",
    "def pyramid(image, scale=1.5, minSize=(30, 30)):\n",
    "    yield image\n",
    " \n",
    "    while True:\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    " \n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "        yield image\n",
    "\n",
    "def sliding_window(image, stepSize, windowSize):\n",
    "    for y in range(0, image.shape[0], stepSize):\n",
    "        for x in range(0, image.shape[1], stepSize):\n",
    "            yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])\n",
    "            \n",
    "def slidingWindow(img):\n",
    "    image = cv2.imread('data/' + path + img +'.jpg')\n",
    "    (winW, winH) = (48, 48)\n",
    "    for resized in pyramid(image, scale=1.5):\n",
    "        for (x, y, window) in sliding_window(resized, stepSize=32, windowSize=(winW, winH)):\n",
    "            if window.shape[0] != winH or window.shape[1] != winW:\n",
    "                continue\n",
    "            box = preProcessImage(window)\n",
    "            features = calculateFeatures([box])\n",
    "            prediction = svmHOG.predict(features[0])\n",
    "            print(prediction)\n",
    "            clone = resized.copy()\n",
    "            #fig,ax = plt.subplots(1)\n",
    "            cv2.rectangle(clone, (x, y), (x + winW, y + winH), (0, 255, 0), 2)\n",
    "            cv2.imshow(\"Window\", clone)\n",
    "            cv2.waitKey(1)\n",
    "            time.sleep(1)\n",
    "            \n",
    "def probability_mass_split(y, folds=7):\n",
    "    obs, classes = y.shape\n",
    "    dist = y.sum(axis=0).astype('float')\n",
    "    dist /= dist.sum()\n",
    "    index_list = []\n",
    "    fold_dist = np.zeros((folds, classes), dtype='float')\n",
    "    for _ in range(folds):\n",
    "        index_list.append([])\n",
    "    for i in range(obs):\n",
    "        if i < folds:\n",
    "            target_fold = i\n",
    "        else:\n",
    "            normed_folds = fold_dist.T / fold_dist.sum(axis=1)\n",
    "            how_off = normed_folds.T - dist\n",
    "            target_fold = np.argmin(np.dot((y[i] - .5).reshape(1, -1), how_off.T))\n",
    "        fold_dist[target_fold] += y[i]\n",
    "        index_list[target_fold].append(i)\n",
    "    print(\"Fold distributions are\")\n",
    "    print(fold_dist)\n",
    "    return index_list\n",
    "\n",
    "def random_sets(y, images, folds):\n",
    "    index_list = []\n",
    "    image_list = []\n",
    "    for _ in range(folds):\n",
    "        index_list.append([])\n",
    "        image_list.append([])\n",
    "    c = list(zip(y, images))\n",
    "    shuffle(c)\n",
    "    y, images = zip(*c)\n",
    "    foldsize = int(len(y)/folds)\n",
    "    idx = 0\n",
    "    for i in range(folds):\n",
    "        if i < folds-1:\n",
    "            index_list[i] = y[idx:idx+foldsize]\n",
    "            image_list[i] = images[idx:idx+foldsize]\n",
    "        else:\n",
    "            index_list[i] = y[idx:]\n",
    "            image_list[i] = images[idx:]\n",
    "        idx += foldsize\n",
    "    return index_list,image_list\n",
    "\n",
    "def getActivations(layer,stimuli):\n",
    "    units = sess.run(layer,feed_dict={x:np.reshape(stimuli,[1,784],order='F'),keep_prob:1.0})\n",
    "    plotNNFilter(units)\n",
    "    \n",
    "def plotNNFilter(units):\n",
    "    filters = units.shape[3]\n",
    "    plt.figure(1, figsize=(20,20))\n",
    "    n_columns = 6\n",
    "    n_rows = math.ceil(filters / n_columns) + 1\n",
    "    for i in range(filters):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.title('Filter ' + str(i))\n",
    "        plt.imshow(units[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")\n",
    "\n",
    "data_folder = 'data'\n",
    "\n",
    "path = \"train2014/COCO_train2014_\"\n",
    "valpath = \"val2014/val2014/COCO_val2014_\"\n",
    "category = \"category_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[58  1  1 ... 85 85 85]\n",
      "17777584\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-737fe265b909>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mlabel_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0msample_idxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_not_processed_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msample_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_idxs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "#def IterativeStratification(labels, r, random_state):\n",
    "num_splits = 5\n",
    "r = np.asarray([1 / num_splits] * num_splits)\n",
    "random_state = check_random_state(42)\n",
    "\n",
    "n_samples = labels.shape[0]\n",
    "test_folds = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "# Calculate the desired number of examples at each subset\n",
    "c_folds = r * n_samples\n",
    "\n",
    "# Calculate the desired number of examples of each label at each subset\n",
    "c_folds_labels = np.outer(r, labels.sum(axis=0))\n",
    "\n",
    "labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n",
    "\n",
    "while np.any(labels_not_processed_mask):\n",
    "    # Find the label with the fewest (but at least one) remaining examples,\n",
    "    # breaking ties randomly\n",
    "    num_labels = labels[labels_not_processed_mask].sum(axis=0)\n",
    "    num_labels = np.asarray(num_labels)\n",
    "    print(type(num_labels))\n",
    "    print(num_labels)\n",
    "    print(num_labels.sum())\n",
    "    # Handle case where only all-zero labels are left by distributing\n",
    "    # across all folds as evenly as possible (not in original algorithm but\n",
    "    # mentioned in the text). (By handling this case separately, some\n",
    "    # code redundancy is introduced; however, this approach allows for\n",
    "    # decreased execution time when there are a relatively large number\n",
    "    # of all-zero labels.)\n",
    "    if num_labels.sum() == 0:\n",
    "        sample_idxs = np.where(labels_not_processed_mask)[0]\n",
    "\n",
    "        for sample_idx in sample_idxs:\n",
    "            fold_idx = np.where(c_folds == c_folds.max())[0]\n",
    "\n",
    "            if fold_idx.shape[0] > 1:\n",
    "                fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n",
    "\n",
    "            test_folds[sample_idx] = fold_idx\n",
    "            c_folds[fold_idx] -= 1\n",
    "\n",
    "        break\n",
    "\n",
    "    label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n",
    "    if label_idx.shape[0] > 1:\n",
    "        label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n",
    "\n",
    "    sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n",
    "\n",
    "    for sample_idx in sample_idxs:\n",
    "        # Find the subset(s) with the largest number of desired examples\n",
    "        # for this label, breaking ties by considering the largest number\n",
    "        # of desired examples, breaking further ties randomly\n",
    "        label_folds = c_folds_labels[:, label_idx]\n",
    "        fold_idx = np.where(label_folds == label_folds.max())[0]\n",
    "\n",
    "        if fold_idx.shape[0] > 1:\n",
    "            temp_fold_idx = np.where(c_folds[fold_idx] ==\n",
    "                                     c_folds[fold_idx].max())[0]\n",
    "            fold_idx = fold_idx[temp_fold_idx]\n",
    "\n",
    "            if temp_fold_idx.shape[0] > 1:\n",
    "                fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n",
    "\n",
    "        test_folds[sample_idx] = fold_idx\n",
    "        labels_not_processed_mask[sample_idx] = False\n",
    "\n",
    "        # Update desired number of examples\n",
    "        c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n",
    "        c_folds[fold_idx] -= 1\n",
    "\n",
    "    #return test_folds\n",
    "\n",
    "#test_folds = IterativeStratification(labels=labelcollection, r=r, random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\olivier.claessen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "#testsplits = list(chunks(dummy_annotations, 50))\n",
    "#labels, features = splitPreprocessing(testsplits, data_folder, path)\n",
    "\n",
    "'''\n",
    "\n",
    "categories_subset = [2,52]\n",
    "bbox = getBoundingBoxesAnnotations(findAnnotations(categories_subset[0], category, train_annotations),  os.path.join(data_folder, path))\n",
    "bbox_val = getBoundingBoxesAnnotations(findAnnotations(categories_subset[0], category, val_annotations),  os.path.join(data_folder, valpath))\n",
    "imgs = bbox[0]\n",
    "imgs_val = bbox_val[0]\n",
    "y_train = bbox[1]\n",
    "y_test = bbox_val[1]\n",
    "categories_subset.pop(0)\n",
    "for category_id in categories_subset:\n",
    "    bbox = getBoundingBoxesAnnotations(findAnnotations(category_id, category, train_annotations),  os.path.join(data_folder, path))\n",
    "    imgs = np.concatenate((imgs,bbox[0]))\n",
    "    y_train = np.concatenate((y_train,bbox[1]))\n",
    "    bbox_val = getBoundingBoxesAnnotations(findAnnotations(category_id, category, val_annotations),  os.path.join(data_folder, valpath))\n",
    "    imgs_val = np.concatenate((imgs_val,bbox_val[0]))\n",
    "    y_test = np.concatenate((y_test,bbox_val[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoundingBoxesPicture(image_id, path):\n",
    "    bounded_images = []\n",
    "    bounded_annotations = []\n",
    "    try:\n",
    "        image_id_string = str(image_id).zfill(12)\n",
    "        image = Image.open(os.path.join(data_folder, path) + image_id_string +'.jpg').convert('RGB')\n",
    "        image = preProcessImage(image) \n",
    "        annotations = findAnnotations(image_id, 'image_id', train_annotations)\n",
    "    except Exception as ex:\n",
    "        print(ex) \n",
    "    for annotation in annotations:\n",
    "        image_resized = resize(getBoundingBox(int(annotation['bbox'][0]),int(annotation['bbox'][1]),int(annotation['bbox'][2]),int(annotation['bbox'][3]),image)\n",
    "                               , (48, 48),\n",
    "                       anti_aliasing=True)\n",
    "        bounded_images.append(image_resized)\n",
    "        bounded_annotations.append(annotation['category_id'])       \n",
    "    bounded_images = np.asarray(bounded_images)\n",
    "    bounded_annotations = np.asarray(bounded_annotations)\n",
    "    return image_id,bounded_images,bounded_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.21611752 0.24495533 0.1768004  ... 0.28481874 0.27704343 0.26100857]\n",
      "  [0.12721626 0.1720974  0.23902519 ... 0.32188185 0.30939419 0.28408601]\n",
      "  [0.17066671 0.3717747  0.52956141 ... 0.32784024 0.30857758 0.28009265]\n",
      "  ...\n",
      "  [0.3516521  0.38121848 0.3675508  ... 0.48588133 0.46274353 0.40846142]\n",
      "  [0.3254989  0.35692239 0.3494512  ... 0.52299184 0.48897578 0.4534547 ]\n",
      "  [0.30203444 0.33799629 0.34615439 ... 0.51888274 0.48679799 0.45085364]]\n",
      "\n",
      " [[0.21650074 0.21538569 0.22596114 ... 0.34805116 0.34928583 0.36014425]\n",
      "  [0.24423325 0.26059546 0.26982914 ... 0.30752242 0.28846537 0.25003961]\n",
      "  [0.21626577 0.30279821 0.29958791 ... 0.28180021 0.28990967 0.23855456]\n",
      "  ...\n",
      "  [0.52637388 0.60627405 0.6441457  ... 0.42914326 0.42291356 0.4010718 ]\n",
      "  [0.2133712  0.2980555  0.47322508 ... 0.39807434 0.34338351 0.317083  ]\n",
      "  [0.18562443 0.19182221 0.19241334 ... 0.32676138 0.33427617 0.20846716]]\n",
      "\n",
      " [[0.29357627 0.33657758 0.34259509 ... 0.53213467 0.53302364 0.49054245]\n",
      "  [0.24188371 0.32177413 0.41246112 ... 0.59162771 0.58529313 0.54299749]\n",
      "  [0.19182251 0.23098838 0.30676892 ... 0.58176864 0.58871579 0.54359189]\n",
      "  ...\n",
      "  [0.5152153  0.54192516 0.51692886 ... 0.53795122 0.50534611 0.46886693]\n",
      "  [0.49645903 0.53998864 0.53020306 ... 0.50999822 0.51843764 0.50295852]\n",
      "  [0.46314512 0.50166044 0.47962644 ... 0.4847483  0.4957365  0.45427829]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.62878253 0.62325046 0.62991461 ... 0.17535105 0.13274361 0.1396083 ]\n",
      "  [0.70437103 0.73449362 0.78846879 ... 0.15112486 0.18507185 0.15775638]\n",
      "  [0.44882659 0.47122646 0.48084557 ... 0.04995575 0.11495022 0.15942597]\n",
      "  ...\n",
      "  [0.54175102 0.54153182 0.55572376 ... 0.54477564 0.5478441  0.55964416]\n",
      "  [0.53147564 0.54221376 0.53986843 ... 0.55512949 0.55366289 0.5651966 ]\n",
      "  [0.47270934 0.48015035 0.4804065  ... 0.49106929 0.49029276 0.50050404]]\n",
      "\n",
      " [[0.23180635 0.18346077 0.22475928 ... 0.33679283 0.33789679 0.34837296]\n",
      "  [0.25852279 0.23692141 0.32225125 ... 0.39595838 0.40337572 0.40646902]\n",
      "  [0.25525944 0.24787704 0.33838561 ... 0.37853293 0.39361251 0.41702205]\n",
      "  ...\n",
      "  [0.51271248 0.53001091 0.52598056 ... 0.21655873 0.21686304 0.22445608]\n",
      "  [0.51345808 0.49211582 0.48640747 ... 0.2255776  0.23786606 0.24461696]\n",
      "  [0.4190906  0.40208108 0.41943418 ... 0.21766843 0.21608812 0.22841295]]\n",
      "\n",
      " [[0.12166763 0.08335993 0.05371356 ... 0.26603669 0.68674364 0.69544115]\n",
      "  [0.16113417 0.14828187 0.11964548 ... 0.35533512 0.70395337 0.73671165]\n",
      "  [0.16682187 0.17042474 0.15836061 ... 0.34403044 0.66541721 0.74105687]\n",
      "  ...\n",
      "  [0.21577588 0.19911074 0.17061257 ... 0.10864668 0.11220399 0.11594622]\n",
      "  [0.17269587 0.15429477 0.1440556  ... 0.11643732 0.10992009 0.11433314]\n",
      "  [0.14039103 0.14517731 0.12994449 ... 0.11740182 0.10976397 0.10704371]]]\n"
     ]
    }
   ],
   "source": [
    "print(images_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(list(itemsets.values()))\n",
    "keys = np.asarray(list(itemsets.keys()))\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "binarized = multilabel_binarizer.fit_transform(labels)\n",
    "\n",
    "rand_sets,rand_images = random_sets(labels,keys,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191188\n",
      "Buffer and memoryview are not contiguous in the same dimension.\n",
      "259687\n",
      "zero-size array to reduction operation minimum which has no identity\n"
     ]
    }
   ],
   "source": [
    "random_images_bboxes = []\n",
    "random_images_bboxes_val = []\n",
    "for image in rand_images[0]:\n",
    "    try:\n",
    "        bbox = getBoundingBoxesPicture(image,path)\n",
    "        random_images_bboxes.append(bbox)\n",
    "    except Exception as ex:\n",
    "        print(image)\n",
    "        print(ex)\n",
    "for image in rand_images[1]:\n",
    "    try:\n",
    "        bbox = getBoundingBoxesPicture(image,path)\n",
    "        random_images_bboxes_val.append(bbox)\n",
    "    except Exception as ex:\n",
    "        print(image)\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id_train, images_train, bounded_annotations_train  = zip(*random_images_bboxes)\n",
    "image_id_val, images_val, bounded_annotations_val  = zip(*random_images_bboxes_val)\n",
    "bounded_annotations_train = list(itertools.chain.from_iterable(bounded_annotations_train))\n",
    "bounded_annotations_val = list(itertools.chain.from_iterable(bounded_annotations_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bounded_annotations_train = [ x for x in bounded_annotations_train if not (x==i).all()]\n",
    "#bounded_annotations_val = [ x for x in bounded_annotations_val if not (x==i).all()]\n",
    "#images_train = [ x for x in images_train if not (x==[]).all()]\n",
    "#images_val = [ x for x in images_val if not (x==[]).all()]\n",
    "'''\n",
    "for idx,image in enumerate(images_train):\n",
    "    if image == []:\n",
    "        images_train.remove(idx)\n",
    "for idx,image in enumerate(images_val):\n",
    "    if image == []:\n",
    "        images_val.remove(idx)\n",
    "'''\n",
    "images_train = images_train[~np.all(images_train == [], axis=0)]\n",
    "images_val = images_val[~np.all(images_val == [], axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16bef8a2550>"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(images_train[1].ndim)\n",
    "fig, ax = plt.subplots(1, figsize=(8, 4))\n",
    "\n",
    "ax.axis('off')\n",
    "ax.imshow(images_train[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = np.vstack(images_train)\n",
    "images_val = np.vstack(images_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000562092885731698, 0.0003632850842555536, 0.0004523573282869526, 0.0025676115394862057, 0.01305433815893622, 0.02169680177804156, 0.009080055055382884, 0.001741334965500611, 0.0053496594714928635, 0.008538572340773914, 0.004611494866589803, 0.003942823609376975, 0.017385605088586706, 0.03340426883613727, 0.016421583113518257, 0.0041220150269409206, 0.004641485360624839, 0.004735706447599752, 0.002116213261856625, 0.00522420414758149, 0.02176052320586591, 0.029126971588919765, 0.010753792806933693, 0.0032599990794713366, 0.0005684849329525348, 0.0003896166698959264, 0.00066558692044777, 0.003994887963503625, 0.015877171541517927, 0.0201310749158181, 0.007128053292510018, 0.0014290401062124322, 0.0006753055736931638, 0.0007315791593936333, 0.00045667352878351757, 0.0014321868179498845, 0.0072647057724548325, 0.012696154635452712, 0.005640184288927874, 0.001239961010571742, 0.0004936302851982971, 0.0005216827053731105, 0.0003947449228740883, 0.0013071284374788312, 0.00695338769726632, 0.013353825473869096, 0.006382159676996992, 0.0012627743159930257, 0.0006959425950841287, 0.00037953743056051465, 0.00033244300190382104, 0.0015332810842717398, 0.008740333241062728, 0.018729366977929526, 0.009996117178638105, 0.0020665156515001634, 0.0058993464912585085, 0.010776473115758611, 0.008273136312152943, 0.0055799296973661275, 0.013431769832798696, 0.023558830502370256, 0.013214010085620578, 0.004034068251658525, 0.01021493623050448, 0.011732993801686497, 0.005515108862226619, 0.005511938455746107, 0.013742585780442186, 0.018251679229433045, 0.010531986308653212, 0.005468304334177666, 0.0022559417706593345, 0.00229826512009815, 0.0017804870134528977, 0.004450118646236973, 0.012735194408401842, 0.015088639785923692, 0.006932058607693826, 0.0023652718534986138, 0.003543482923900779, 0.005275079439895316, 0.0025831629575151444, 0.0018453967764624727, 0.0054757383236182485, 0.0085478017636898, 0.0040145194437691805, 0.0015105163156077836, 0.00411424893520334, 0.008557569756129962, 0.00475504679738172, 0.0016163455686463064, 0.002878502073846002, 0.005487145280576923, 0.0034013249471213825, 0.0015401041963682839, 0.0020003759316336196, 0.003239475286742835, 0.002084664231865723, 0.0013591158014821, 0.0055484882602696625, 0.015248155352729538, 0.011678292421196139, 0.0032605315787544993, 0.005415402595696728, 0.0078079662601193015, 0.006539232826927847, 0.0049286574014202325, 0.00980623215423835, 0.017644521598618138, 0.013230183036629435, 0.005509137241400688, 0.005830438389818208, 0.006708754304861549, 0.0047690165498094385, 0.0057057116860499265, 0.012089725365653378, 0.017227275184766223, 0.011389869371030207, 0.004960189434524213, 0.0026397651079535455, 0.002797200803616387, 0.0022569889400299126, 0.004908255647082951, 0.011979707143772084, 0.012984241729837927, 0.006328038788559201, 0.002490625005750256, 0.003612486106170532, 0.005476225788078272, 0.0029372859852842444, 0.002140261781938922, 0.00535903046403887, 0.008015154596324976, 0.004169411754801177, 0.001685150211107817, 0.003545837762644224, 0.007039656259106747, 0.0040241957226479115, 0.0017211653087845602, 0.0034380105018188955, 0.006264579883966118, 0.004114641812634101, 0.0017113291365726873, 0.0024175506465989607, 0.003986820916363051, 0.0026256073947388157, 0.0015336426848708902, 0.005221638452715207, 0.014628102840213927, 0.012782263911374828, 0.004018321792834049]\n"
     ]
    }
   ],
   "source": [
    "print(full_picture_features[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = LinearSVC()\n",
    "ovr = OneVsRestClassifier(clf)\n",
    "full_picture_features = calculateFeatures(images_train) \n",
    "full_picture_features_val = calculateFeatures(images_val) \n",
    "picture_features = [np.concatenate((x,y)) for x,y in zip(full_picture_features[0], full_picture_features[1])]\n",
    "picture_features_val = [np.concatenate((x,y)) for x,y in zip(full_picture_features_val[0], full_picture_features_val[1])]\n",
    "svm = svmFit(picture_features, bounded_annotations_train)\n",
    "svmResults = svmPredict(picture_features_val, bounded_annotations_val, svm)\n",
    "\n",
    "#features_concat = [np.concatenate((x,y,z)) for x,y,z in zip(full_picture_features[0], full_picture_features[1], full_picture_features[2])]\n",
    "#features_val_concat = [np.concatenate((x,y,z)) for x,y,z in zip(full_picture_features_val[0], full_picture_features_val[1], full_picture_features_val[2])]\n",
    "ovr.fit(full_picture_features[0], bounded_annotations_train)\n",
    "Y_pred_ovr = ovr.score(full_picture_features_val[0], bounded_annotations_val)\n",
    "print(Y_pred_ovr)\n",
    "Y_pred_ovr = ovr.predict(full_picture_features_val[0])\n",
    "ovr_jaccard_score = jaccard_similarity_score(bounded_annotations_val, Y_pred_ovr)\n",
    "\n",
    "# Fit an ensemble of logistic regression classifier chains and take the\n",
    "# take the average prediction of all the chains.\n",
    "chains = [ClassifierChain(clf, random_state=i)\n",
    "          for i in range(90)]\n",
    "for chain in chains:\n",
    "    chain.fit(full_picture_features[0], bounded_annotations_train)\n",
    "\n",
    "Y_pred_chains = np.array([chain.predict(features_val_concat[0]) for chain in\n",
    "                          chains])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx_trainfeat, x_testfeat, y_trainfeat, y_testfeat = train_test_split(\\n    features, y_train, stratify=y_train,  test_size=0.25, random_state=42)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = calculateFeatures(imgs)\n",
    "features_val = calculateFeatures(imgs_val)\n",
    "\n",
    "'''\n",
    "x_trainfeat, x_testfeat, y_trainfeat, y_testfeat = train_test_split(\n",
    "    features, y_train, stratify=y_train,  test_size=0.25, random_state=42)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_concat = [np.concatenate((x,y,z,a)) for x,y,z,a in zip(features[0], features[1], features[2], features[3])]\n",
    "features_val_concat = [np.concatenate((x,y,z,a)) for x,y,z,a in zip(features_val[0], features_val[1], features_val[2],features_val[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to train\n",
      "Predicting the test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.73      0.76      0.74      2474\n",
      "          52       0.79      0.77      0.78      2925\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      5399\n",
      "   macro avg       0.76      0.76      0.76      5399\n",
      "weighted avg       0.76      0.76      0.76      5399\n",
      "\n",
      "[[1870  604]\n",
      " [ 684 2241]]\n"
     ]
    }
   ],
   "source": [
    "#svm = svmFit(features_concat, y_train)\n",
    "#svmResults = svmPredict(features_val_concat, y_test, svm)\n",
    "\n",
    "\n",
    "svmHOG = svmFit(features[0], y_train)\n",
    "svmHOGResults = svmPredict(features_val[0], y_test, svmHOG)\n",
    "#saveModel(svm, 'SVM_DAISY.sav')\n",
    "#os.makedirs('outputs', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slidingWindow(\"000000002664\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to train\n",
      "Completed in 161.189s\n",
      "Predicting the test set\n",
      "Completed in 4.056s\n",
      "Accuracy: 0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "rfHOG = randomForestFit(features_concat, y_train, 1000)\n",
    "rfHOG_results = rfPredict(features_val_concat, y_test, rfHOG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width 578.2023976316078\n",
    "# heigth 483.5494085111049\n",
    "\n",
    "path = \"data/train2014/COCO_train2014_\"\n",
    "def getPictures(itemsets, path, instance):\n",
    "    images = []\n",
    "    image_ids = []\n",
    "    image_tags = []\n",
    "    avg_width, avg_height = (0,0)\n",
    "    try:\n",
    "        for itemset in itemsets:\n",
    "            print(itemset)\n",
    "            print(itemsets[itemset])\n",
    "            if(instance in itemsets[itemset]):\n",
    "                image_id_string = str(itemset).zfill(12)\n",
    "                image_ids.append(str(itemset))\n",
    "                image = Image.open(path + image_id_string +'.jpg')\n",
    "                processed_img = preProcessImage(image)\n",
    "                image.close()\n",
    "                resized_image = resize(processed_img, (350, 292),anti_aliasing=True)\n",
    "                images.append(resized_image)\n",
    "                image_tags.append(itemsets[itemset])\n",
    "    except Exception as ex:\n",
    "            print(ex)   \n",
    "    return image_ids,images,image_tags\n",
    "\n",
    "pictureData = getPictures(itemsets, path, 18)\n",
    "#print(pictureData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\olivier.claessen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "c:\\users\\olivier.claessen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:334: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "def example_errors():\n",
    "    generator_test.reset()\n",
    "    \n",
    "    # Predict the classes for all images in the test-set.\n",
    "    y_pred = new_model.predict_generator(generator_test,\n",
    "                                         steps=steps_test)\n",
    "\n",
    "    # Convert the predicted classes from arrays to integers.\n",
    "    cls_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "    # Plot examples of mis-classified images.\n",
    "    plot_example_errors(cls_pred)\n",
    "    \n",
    "    # Print the confusion matrix.\n",
    "    print_confusion_matrix(cls_pred)\n",
    "categories_subset = [2,6,19]\n",
    "bbox = getBoundingBoxesAnnotations(findAnnotations(categories_subset[0], category, train_annotations),  os.path.join(data_folder, path))\n",
    "bbox_val = getBoundingBoxesAnnotations(findAnnotations(categories_subset[0], category, val_annotations),  os.path.join(data_folder, valpath))\n",
    "imgs = bbox[0]\n",
    "imgs_val = bbox_val[0]\n",
    "y_train = bbox[1]\n",
    "y_test = bbox_val[1]\n",
    "categories_subset.pop(0)\n",
    "for category_id in categories_subset:\n",
    "    bbox = getBoundingBoxesAnnotations(findAnnotations(category_id, category, train_annotations),  os.path.join(data_folder, path))\n",
    "    imgs = np.concatenate((imgs,bbox[0]))\n",
    "    y_train = np.concatenate((y_train,bbox[1]))\n",
    "    bbox_val = getBoundingBoxesAnnotations(findAnnotations(category_id, category, val_annotations),  os.path.join(data_folder, valpath))\n",
    "    imgs_val = np.concatenate((imgs_val,bbox_val[0]))\n",
    "    y_test = np.concatenate((y_test,bbox_val[1]))\n",
    "\n",
    "#images = np.asarray(pictureData[1])\n",
    "\n",
    "'''\n",
    "#x_train, x_test, y_train, y_test = train_test_split(\n",
    "#    imgs, y_train, test_size=0.25, random_state=42)\n",
    "'''\n",
    "x_train = imgs/255\n",
    "x_test = imgs_val/255\n",
    "\n",
    "for (i,value) in enumerate(y_train):\n",
    "    if value == 2:\n",
    "        y_train[i] = 0\n",
    "        continue\n",
    "    if value == 6:\n",
    "        y_train[i] = 1\n",
    "        continue\n",
    "    if value == 19:\n",
    "        y_train[i] = 2\n",
    "        continue\n",
    "    if value == 20:\n",
    "        y_train[i] = 3\n",
    "        continue\n",
    "    if value == 33:\n",
    "        y_train[i] = 4\n",
    "        continue\n",
    "    if value == 46:\n",
    "        y_train[i] = 5\n",
    "        continue\n",
    "    if value == 49:\n",
    "        y_train[i] = 6\n",
    "        continue\n",
    "    if value == 55:\n",
    "        y_train[i] = 7\n",
    "        continue\n",
    "for (i,value) in enumerate(y_test):\n",
    "    if value == 2:\n",
    "        y_test[i] = 0\n",
    "        continue\n",
    "    if value == 6:\n",
    "        y_test[i] = 1\n",
    "        continue\n",
    "    if value == 19:\n",
    "        y_test[i] = 2\n",
    "        continue\n",
    "    if value == 20:\n",
    "        y_test[i] = 3\n",
    "        continue\n",
    "    if value == 33:\n",
    "        y_test[i] = 4\n",
    "        continue\n",
    "    if value == 46:\n",
    "        y_test[i] = 5\n",
    "        continue\n",
    "    if value == 49:\n",
    "        y_test[i] = 6\n",
    "        continue\n",
    "    if value == 55:\n",
    "        y_test[i] = 7\n",
    "        continue\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    zca_whitening=True, \n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "datagen.fit(x_train)\n",
    "datagen_test = ImageDataGenerator(zca_whitening=True)\n",
    "datagen_test.fit(x_test)         \n",
    "\n",
    "ylabels_test = to_categorical(y_test)\n",
    "ylabels_train = to_categorical(y_train)\n",
    "#joblib.dump(value=ylabels_train, filename='outputs/ylabels_train2,6,19,20,33,46,49,55c.npy')\n",
    "#joblib.dump(value=ylabels_test, filename='outputs/ylabels_test2,6,19,20,33,46,49,55c.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = joblib.load('outputs/trainset2,6,19,20,33,46,49,55.npy')\n",
    "x_test = joblib.load('outputs/testset2,6,19,20,33,46,49,55.npy')\n",
    "#ylabels_train = joblib.load('outputs/y_trainlabels2,6,19,20,33,46.npy')\n",
    "#ylabels_test = joblib.load('outputs/y_testlabels2,6,19,20,33,46.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\olivier.claessen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:339: UserWarning: This ImageDataGenerator specifies `zca_whitening` which overrides setting of`featurewise_std_normalization`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True, \n",
    "    zca_whitening=True,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "datagen.fit(x_train)\n",
    "datagen_test = ImageDataGenerator(featurewise_center=True, \n",
    "    zca_whitening=True,)\n",
    "datagen_test.fit(x_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x3x3x128,2x3x3x64,0.25,D64,0.25 model.fit\n",
      "Train on 30471 samples, validate on 14793 samples\n",
      "WARNING:tensorflow:From c:\\users\\olivier.claessen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\olivier.claessen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  910/30471 [..............................] - ETA: 53:53 - loss: 1.9417 - acc: 0.1802"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-304fe4c953bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2x3x3x128,2x3x3x64,0.25,D64,0.25 model.fit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylabels_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\olivier.claessen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\olivier.claessen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\olivier.claessen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32mc:\\users\\olivier.claessen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "early_stopping_monitor = EarlyStopping(patience = 3)\n",
    "\n",
    "\n",
    "'''\n",
    "dense_layers = [1]\n",
    "layer_sizes = [64]\n",
    "dense_sizes = [64]\n",
    "conv_layers = [1]\n",
    "kernel_sizes = [3]\n",
    "for dense_layer in dense_layers:\n",
    "    for dense_size in dense_sizes:\n",
    "        for kernel_size in kernel_sizes:\n",
    "            for layer_size in layer_sizes:\n",
    "                for conv_layer in conv_layers:\n",
    "                    model = constructCNN(conv_layer, layer_size, kernel_size, dense_size, dense_layer, True, 3, x_train)\n",
    "                    NAME = \"{}-conv-{}-nodes-{}-dense-{}-dense_size-{}-kernel-{}\".format(conv_layer, layer_size, dense_layer, dense_size, kernel_size, int(time()))\n",
    "                    print(NAME)\n",
    "                    tensorboard = TensorBoard(log_dir=\"logs2,6,19,20,33,46,49,55c/{}\".format(NAME))\n",
    "                    #model.fit(x_train, ylabels_train.astype(np.float32), batch_size=70, epochs=100, validation_data = (x_test, ylabels_test), callbacks = [tensorboard],shuffle=True)\n",
    "                    model.fit_generator(datagen.flow(x_train, ylabels_train, batch_size=32,shuffle=True), \n",
    "                                        epochs=15, callbacks = [early_stopping_monitor], validation_data = (x_test, ylabels_test))\n",
    "\n",
    "'''\n",
    "'''\n",
    "y_pred = model.predict_classes(x_test)\n",
    "true_preds = [(x,y) for (x,y,p) in zip(X_test, y_test, y_pred) if y == p]\n",
    "false_preds = [(x,y,p) for (x,y,p) in zip(X_test, y_test, y_pred) if y != p]\n",
    "print(\"Number of true predictions: \", len(true_preds))\n",
    "print(\"Number of false predictions:\", len(false_preds))\n",
    "\n",
    "'''\n",
    "percent_noise = 0.1\n",
    "noise = (1.0/255) * percent_noise\n",
    "model = Sequential()\n",
    "model.add(GaussianNoise(noise, input_shape=x_train.shape[1:]))\n",
    "model.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "        \n",
    "print(\"2x3x3x128,2x3x3x64,0.25,D64,0.25 model.fit\")\n",
    "model.fit(x_train, ylabels_train.astype(np.float32), batch_size=70, epochs=100, validation_data = (x_test, ylabels_test), callbacks = [],shuffle=True)\n",
    "model.save_weights('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30471, 64, 64, 3)\n",
      "(14793, 8)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(ylabels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./coco-multi-label/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Run\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import os\n",
    "from PIL import Image, ImageFilter\n",
    "from skimage import io as io\n",
    "from skimage import exposure\n",
    "from skimage.transform import resize, integral_image\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import blob_dog, blob_log, blob_doh, corner_harris, corner_subpix, corner_peaks, daisy, hog, multiblock_lbp, haar_like_feature\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import time\n",
    "\n",
    "def loadAnnotations(fileLocation):\n",
    "    with open(fileLocation) as read_file:\n",
    "        annotations = json.load(read_file)\n",
    "    return annotations\n",
    "    \n",
    "def getBoundingBox(x,y,w,h,img):\n",
    "    return img[y:y+h,x:x+w]\n",
    "\n",
    "def findAnnotations(id, property, annotations):\n",
    "    items = []\n",
    "    for annotation in annotations: \n",
    "        if annotation[property] == id:\n",
    "            items.append(annotation)\n",
    "    return items\n",
    "\n",
    "def zca_whitening_matrix(X):\n",
    "    # Covariance matrix [column-wise variables]: Sigma = (X-mu)' * (X-mu) / N\n",
    "    sigma = np.cov(X, rowvar=True) # [M x M]\n",
    "    # Singular Value Decomposition. X = U * np.diag(S) * V\n",
    "    U,S,V = np.linalg.svd(sigma)\n",
    "        # U: [M x M] eigenvectors of sigma.\n",
    "        # S: [M x 1] eigenvalues of sigma.\n",
    "        # V: [M x M] transpose of U\n",
    "    # Whitening constant: prevents division by zero\n",
    "    epsilon = 1e-5\n",
    "    # ZCA Whitening matrix: U * Lambda * U'\n",
    "    ZCAMatrix = np.dot(U, np.dot(np.diag(1.0/np.sqrt(S + epsilon)), U.T)) # [M x M]\n",
    "    return ZCAMatrix\n",
    "\n",
    "def preProcessImage(img):\n",
    "    img = np.asarray(img)\n",
    "    img = rgb2gray(img)\n",
    "    #img = cv2.GaussianBlur(img,(5,5),0)\n",
    "    #img = cv2.medianBlur(img,5)\n",
    "    #img = cv2.bilateralFilter(img,9,75,75)\n",
    "    #img = cv2.blur(img,(5,5))\n",
    "    #kernel = np.ones((5,5),np.float32)/25\n",
    "    #img = cv2.filter2D(img,-1,kernel)\n",
    "    return img\n",
    "\n",
    "def noisy(image):\n",
    "    row,col,ch = image.shape\n",
    "    s_vs_p = 0.5\n",
    "    amount = 0.004\n",
    "    out = np.copy(image)\n",
    "    # Salt mode\n",
    "    num_salt = np.ceil(amount * image.size * s_vs_p)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "          for i in image.shape]\n",
    "    out[coords] = 1\n",
    "\n",
    "    # Pepper mode\n",
    "    num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))\n",
    "    coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "          for i in image.shape]\n",
    "    out[coords] = 0\n",
    "    return out\n",
    "\n",
    "def getBoundingBoxesAnnotations(annotations, path):\n",
    "    bounded_images = []\n",
    "    bounded_annotations = []\n",
    "    try:\n",
    "        for annotation in annotations:\n",
    "            image_id = annotation['image_id']\n",
    "            image_id_string = str(image_id).zfill(12)\n",
    "            image = Image.open(path + image_id_string +'.jpg').convert('RGB')\n",
    "            image = preProcessImage(image) \n",
    "            image_resized = resize(getBoundingBox(int(annotation['bbox'][0]),int(annotation['bbox'][1]),int(annotation['bbox'][2]),int(annotation['bbox'][3]),image)\n",
    "                                   , (48, 48),\n",
    "                           anti_aliasing=True)\n",
    "            bounded_images.append(image_resized)\n",
    "            bounded_annotations.append(annotation['category_id'])     \n",
    "    except Exception as ex:\n",
    "            print(ex)   \n",
    "    bounded_images = np.asarray(bounded_images)\n",
    "    bounded_annotations = np.asarray(bounded_annotations)\n",
    "    return bounded_images,bounded_annotations\n",
    "\n",
    "def calculateHogFeatures(gray_image, o, pixels, cells):\n",
    "    features = hog(gray_image, orientations=o, \n",
    "                              pixels_per_cell=(pixels, pixels),\n",
    "                              cells_per_block=(cells, cells), \n",
    "                              transform_sqrt=True, \n",
    "                              visualize=False, block_norm = \"L2-Hys\")\n",
    "    return features\n",
    "def calculateDaisyFeatures(gray_image):\n",
    "    descs = daisy(gray_image, step=180, radius=8, rings=3, histograms=6,\n",
    "                         orientations=8, visualize=False)\n",
    "    descs_num = descs.shape[0] * descs.shape[1]\n",
    "    return descs.reshape(descs.size).tolist()\n",
    "\n",
    "def calculateDoG(gray_image):\n",
    "    blobs_dog = blob_dog(gray_image, max_sigma=30, threshold=.1)\n",
    "    blobs_dog[:, 2] = blobs_dog[:, 2] * sqrt(2)\n",
    "    return blobs_dog\n",
    "\n",
    "def calculateSIFT(sift, gray_image):\n",
    "    kp = sift.detect(gray_image,None)\n",
    "    return kp\n",
    "\n",
    "def calculateFeatures(imgs):\n",
    "    #sift = cv2.xfeatures2d.SIFT_create()\n",
    "    #detector = CENSURE()\n",
    "    hog_features = []\n",
    "    dog_features = []\n",
    "    daisy_features = []\n",
    "    sift_features = []\n",
    "    lbp_features = []\n",
    "    haar_features = []\n",
    "    i = 0;\n",
    "    for img in imgs:\n",
    "        hog_features.append(calculateHogFeatures(img,8,16,1))\n",
    "        daisy_features.append(calculateDaisyFeatures(img))\n",
    "        int_img = integral_image(img)\n",
    "        lbp_features.append(multiblock_lbp(int_img, 0, 0, 3, 3))\n",
    "        haar_features.append(haar_like_feature(int_img, 0, 0, 48, 48, 'type-3-x'))\n",
    "        #shape_index_features.append(shape_index(img))\n",
    "        #dog_features.append(calculateDoG(img))\n",
    "        #harris_features.append(corner_peaks(corner_harris(img), min_distance=5))\n",
    "        #sift_features.append(calculateSIFT(sift, img))\n",
    "    return hog_features, daisy_features, lbp_features, haar_features\n",
    "\n",
    "def svmFit(x_train, y_train):\n",
    "    print(\"Fitting the classifier to train\")\n",
    "    t0 = time()\n",
    "    '''\n",
    "    param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "                  'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "    clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'),\n",
    "                       param_grid, cv=5)\n",
    "    '''\n",
    "    clf = LinearSVC()\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    print(\"Completed in %0.3fs\" % (time() - t0))\n",
    "    #print(\"Best estimator found by grid search:\")\n",
    "    #print(clf.best_estimator_)\n",
    "    return clf\n",
    "\n",
    "def saveModel(model, filename):\n",
    "    print(\"Saving file...\")\n",
    "    joblib.dump(model, open(filename, 'wb'))\n",
    "    print(\"File saved\")\n",
    "    \n",
    "def loadModel(filename):\n",
    "    print(\"loading file...\")\n",
    "    joblib.load(filename)\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "def svmPredict(x_test, y_test, model):\n",
    "    print(\"Predicting the test set\")\n",
    "    t0 = time()\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"Completed in %0.3fs\" % (time() - t0))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "def randomForestFit(x_train, y_train, estimators):\n",
    "    print(\"Fitting the classifier to train\")\n",
    "    t0 = time()\n",
    "    rf = RandomForestClassifier(n_estimators=estimators)\n",
    "    rf.fit(x_train, y_train);\n",
    "    print(\"Completed in %0.3fs\" % (time() - t0))\n",
    "    return rf\n",
    "\n",
    "def rfPredict(x_test, y_test, model):\n",
    "    print(\"Predicting the test set\")\n",
    "    t0 = time()\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"Completed in %0.3fs\" % (time() - t0))\n",
    "    errors = abs(y_pred - y_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "# Create a function called \"chunks\" with two arguments, l and n:\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]\n",
    "\n",
    "def splitPreprocessing(splits, data_folder, path):\n",
    "    labels = []\n",
    "    features = np.asarray([[],[]])\n",
    "    for split in splits:\n",
    "        bboxes = getBoundingBoxesAnnotations(split, os.path.join(data_folder, path))\n",
    "        splitfeatures = calculateFeatures(bboxes[0])\n",
    "        features = np.append(features, splitfeatures,1)\n",
    "        labels.extend(bboxes[1])\n",
    "    return labels, features.tolist()\n",
    "\n",
    "def non_max_suppression_slow(boxes, overlapThresh):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    " \n",
    "    no_overlap = []\n",
    " \n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    " \n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "    while len(idxs) > 0:\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        no_overlap.append(i)\n",
    " \n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    " \n",
    "\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    " \n",
    "    return boxes[pick].astype(\"int\")\n",
    "\n",
    "def IoU(box_1,box_2):\n",
    "    x_a = max(box_1[0],box_2[0])\n",
    "    y_a = max(box_1[1],box_2[1])\n",
    "    x_b = min(box_1[2],box_2[2])\n",
    "    y_b = min(box_1[3],box_2[3])\n",
    "    \n",
    "    overlap = max(0, x_b - x_a + 1) * max(0, y_b - y_a + 1)\n",
    "    box_1_area = (box_1[2] - box_1[0] + 1) * (box_1[3] - box_1[1] + 1)\n",
    "    box_2_area = (box_2[2] - box_2[0] + 1) * (box_2[3] - box_2[1] + 1)\n",
    "    IoU = overlap/float(box_1_area + box_2_area + overlap)\n",
    "    \n",
    "    return IoU\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "args = parser.parse_args()\n",
    "data_folder = os.path.join(args.data_folder, 'data')\n",
    "print('Data folder:', data_folder)\n",
    "run = Run.get_context()\n",
    "\n",
    "path = \"train2014/COCO_train2014_\"\n",
    "valpath = \"val2014/val2014/COCO_val2014_\"\n",
    "category = \"category_id\"\n",
    "\n",
    "with open(os.path.join(data_folder, \"annotations/instances_train2014.json\")) as read_file:\n",
    "    train = json.load(read_file)\n",
    "\n",
    "with open(os.path.join(data_folder, \"annotations/instances_val2014.json\")) as read_file:\n",
    "    val = json.load(read_file)\n",
    "\n",
    "train_annotations = train[\"annotations\"]\n",
    "val_annotations = val[\"annotations\"]\n",
    "categories_subset = [2,6]\n",
    "bbox = getBoundingBoxesAnnotations(findAnnotations(categories_subset[0], category, train_annotations),  os.path.join(data_folder, path))\n",
    "bbox_val = getBoundingBoxesAnnotations(findAnnotations(categories_subset[0], category, val_annotations),  os.path.join(data_folder, valpath))\n",
    "imgs = bbox[0]\n",
    "imgs_val = bbox_val[0]\n",
    "y_train = bbox[1]\n",
    "y_test = bbox_val[1]\n",
    "categories_subset.pop(0)\n",
    "for category_id in categories_subset:\n",
    "    bbox = getBoundingBoxesAnnotations(findAnnotations(category_id, category, train_annotations),  os.path.join(data_folder, path))\n",
    "    imgs = np.concatenate((imgs,bbox[0]))\n",
    "    y_train = np.concatenate((y_train,bbox[1]))\n",
    "    bbox_val = getBoundingBoxesAnnotations(findAnnotations(category_id, category, val_annotations),  os.path.join(data_folder, valpath))\n",
    "    imgs_val = np.concatenate((imgs_val,bbox_val[0]))\n",
    "    y_test = np.concatenate((y_test,bbox_val[1]))\n",
    "    \n",
    "joblib.dump(value=imgs, filename='outputs/bounded_imgs2,6,19,20,33,46,49,55.npy')\n",
    "joblib.dump(value=imgs_val, filename='outputs/bounded_imgs_val2,6,19,20,33,46,49,55.npy')\n",
    "features = calculateFeatures(imgs)\n",
    "features_val = calculateFeatures(imgs_val)\n",
    "joblib.dump(value=features, filename='outputs/features2,6,19,20,33,46,49,55.npy')\n",
    "joblib.dump(value=features_val, filename='outputs/features_val2,6,19,20,33,46,49,55.npy')\n",
    "features_concat = [np.concatenate((x,y,z,a)) for x,y,z,a in zip(features[0], features[1], features[2], features[3])]\n",
    "features_val_concat = [np.concatenate((x,y,z,a)) for x,y,z,a in zip(features_val[0], features_val[1], features_val[2],features_val[3])]\n",
    "print(\"SVM\")\n",
    "svm = svmFit(features_concat, y_train)\n",
    "svm_results = svmPredict(features_val_concat, y_test, svm)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=svm, filename='outputs/SVM.pkl')\n",
    "print(\"RF\")\n",
    "rf = randomForestFit(features_concat, y_train, 1000)\n",
    "rf_results = rfPredict(features_val_concat, y_test, rfHOG)\n",
    "joblib.dump(value=rf, filename='outputs/rf.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./coco-multi-label/neural_network.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/neural_network.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Run\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import os\n",
    "from PIL import Image, ImageFilter\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Activation, Flatten, Dropout, GaussianNoise, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def loadAnnotations(fileLocation):\n",
    "    with open(fileLocation) as read_file:\n",
    "        annotations = json.load(read_file)\n",
    "    return annotations\n",
    "    \n",
    "def getBoundingBox(x,y,w,h,img):\n",
    "    return img[y:y+h,x:x+w]\n",
    "\n",
    "def findAnnotations(id, property, annotations):\n",
    "    items = []\n",
    "    for annotation in annotations: \n",
    "        if annotation[property] == id:\n",
    "            items.append(annotation)\n",
    "    return items\n",
    "\n",
    "def preProcessImage(img):\n",
    "    img = np.asarray(img)\n",
    "    return img\n",
    "\n",
    "def getBoundingBoxesAnnotations(annotations, path):\n",
    "    bounded_images = []\n",
    "    bounded_annotations = []\n",
    "    try:\n",
    "        for annotation in annotations:\n",
    "            image_id = annotation['image_id']\n",
    "            image_id_string = str(image_id).zfill(12)\n",
    "            image = Image.open(path + image_id_string +'.jpg').convert('RGB')\n",
    "            image = preProcessImage(image) \n",
    "            image_resized = resize(getBoundingBox(int(annotation['bbox'][0]),int(annotation['bbox'][1]),int(annotation['bbox'][2]),int(annotation['bbox'][3]),image)\n",
    "                                   , (48, 48),\n",
    "                           anti_aliasing=True)\n",
    "            bounded_images.append(image_resized)\n",
    "            bounded_annotations.append(annotation['category_id'])     \n",
    "    except Exception as ex:\n",
    "            print(ex)   \n",
    "    bounded_images = np.asarray(bounded_images)\n",
    "    bounded_annotations = np.asarray(bounded_annotations)\n",
    "    return bounded_images,bounded_annotations\n",
    "\n",
    " \n",
    "def constructCNN(conv_layer, layer_size, kernel_size, dense_size, dense_layer, dropout, num_classes, x_train):\n",
    "    percent_noise = 0.1\n",
    "    noise = (1.0/255) * percent_noise\n",
    "    model = Sequential()\n",
    "    model.add(GaussianNoise(noise, input_shape=x_train.shape[1:]))\n",
    "    model.add(Conv2D(layer_size, (kernel_size, kernel_size)))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    for l in range(conv_layer-1):\n",
    "        model.add(Conv2D(layer_size, (kernel_size, kernel_size)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    if(dropout):\n",
    "        model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    for j in range(dense_layer):\n",
    "        model.add(Dense(dense_size))\n",
    "        model.add(Activation('relu'))\n",
    "    if(dropout):\n",
    "        model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "args = parser.parse_args()\n",
    "data_folder = os.path.join(args.data_folder, 'outputs')\n",
    "print('Data folder:', data_folder)\n",
    "run = Run.get_context()\n",
    "\n",
    "path = \"train2014/COCO_train2014_\"\n",
    "category = \"category_id\"\n",
    "x_train = joblib.load(os.path.join(data_folder, 'trainset2,6,19,20,33,46,49,55c.npy'))\n",
    "x_test = joblib.load(os.path.join(data_folder, 'testset2,6,19,20,33,46,49,55c.npy'))\n",
    "ylabels_train = joblib.load(os.path.join(data_folder, 'ylabels_train2,6,19,20,33,46,49,55.npy'))\n",
    "ylabels_test = joblib.load(os.path.join(data_folder, 'ylabels_test2,6,19,20,33,46,49,55.npy'))\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True, \n",
    "    zca_whitening=True,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "datagen.fit(x_train)\n",
    "datagen_test = ImageDataGenerator(featurewise_center=True, \n",
    "    zca_whitening=True,)\n",
    "datagen_test.fit(x_test)       \n",
    "\n",
    "sess = tf.Session()\n",
    "early_stopping_monitor = EarlyStopping(patience = 3)\n",
    "\n",
    "dense_layers = [3]\n",
    "layer_sizes = [128]\n",
    "dense_sizes = [128]\n",
    "conv_layers = [4]\n",
    "kernel_sizes = [3]\n",
    "for dense_layer in dense_layers:\n",
    "    for dense_size in dense_sizes:\n",
    "        for kernel_size in kernel_sizes:\n",
    "            for layer_size in layer_sizes:\n",
    "                for conv_layer in conv_layers:\n",
    "                    model = constructCNN(conv_layer, layer_size, kernel_size, dense_size, dense_layer, True, 8, x_train)\n",
    "                    NAME = \"{}-conv-{}-nodes-{}-dense-{}-dense_size-{}-kernel\".format(conv_layer, layer_size, dense_layer, dense_size, kernel_size)\n",
    "                    print(NAME)\n",
    "                    tensorboard = TensorBoard(log_dir=\"logs2,6,19,20,33,46,49,55c/{}\".format(NAME))\n",
    "                    #model.fit(x_train, ylabels_train.astype(np.float32), batch_size=70, epochs=100, validation_data = (x_test, ylabels_test), callbacks = [tensorboard],shuffle=True)\n",
    "                    model.fit_generator(datagen.flow(x_train, ylabels_train, batch_size=32,shuffle=True), \n",
    "                    epochs=100, callbacks = [], validation_data = datagen_test.flow(x_test, ylabels_test, shuffle=True))\n",
    "model.save_weights('model.h5')\n",
    "'''\n",
    "percent_noise = 0.1\n",
    "noise = (1.0/255) * percent_noise\n",
    "model = Sequential()\n",
    "model.add(GaussianNoise(noise, input_shape=x_train.shape[1:]))\n",
    "model.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "        \n",
    "print(\"2x3x3x128,2x3x3x64,0.25,D64,0.25 model.fit\")\n",
    "model.fit_generator(datagen.flow(x_train, ylabels_train, batch_size=32,shuffle=True), \n",
    "                    epochs=100, callbacks = [], validation_data = datagen_test.flow(x_test, ylabels_test, shuffle=True))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./coco-multi-label\\\\utils.py'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.copy('utils.py', script_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "}\n",
    "\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                conda_packages=['scikit-learn', 'scikit-image', 'pillow', 'opencv', 'numpy'])\n",
    "\n",
    "run = exp.submit(config=est)\n",
    "run\n",
    "'''\n",
    "\n",
    "keras_est = TensorFlow(source_directory=script_folder,\n",
    "                       script_params=script_params,\n",
    "                       compute_target=compute_target,\n",
    "                       entry_script='neural_network.py',\n",
    "                       pip_packages=['keras', 'scikit-learn', 'scikit-image', 'pillow'],\n",
    "                      use_gpu=True)\n",
    "\n",
    "run = exp.submit(config=keras_est)\n",
    "run\n",
    "'''\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()\n",
    "\n",
    "run.wait_for_completion(show_output=True) # specify True for a verbose log\n",
    "\n",
    "print(run.get_metrics())\n",
    "\n",
    "print(run.get_file_names())\n",
    "\n",
    "# register model \n",
    "#model = run.register_model(model_name='svm_hog', model_path='outputs/svm_hog.pkl')\n",
    "#print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-class_classification_1553865295_69cd11d7 Running\n",
      "multi-class_classification_1553865088_1aa09112 Running\n",
      "multi-class_classification_1553864091_b5d7a8f0 Completed\n",
      "multi-class_classification_1553862335_1cc89eea Canceled\n",
      "multi-class_classification_1553861876_d7b23c93 Canceled\n",
      "multi-class_classification_1553861597_9c5e9bb1 Completed\n",
      "multi-class_classification_1553861290_d368a010 Canceled\n",
      "multi-class_classification_1553860670_3301962e Completed\n",
      "multi-class_classification_1553860550_0d3885a6 Completed\n",
      "multi-class_classification_1553860374_5df4ed84 Canceled\n",
      "multi-class_classification_1553860236_685bc22d Completed\n",
      "multi-class_classification_1553859814_0f6fbc92 Completed\n",
      "multi-class_classification_1553859524_df3f497b Completed\n",
      "multi-class_classification_1553858058_125fcfa9 Canceled\n",
      "multi-class_classification_1553856813_a97f9ba4 Completed\n",
      "multi-class_classification_1553782189_1323af86 Failed\n",
      "multi-class_classification_1553780005_dac75afb Completed\n",
      "multi-class_classification_1553776065_265e76df Completed\n",
      "multi-class_classification_1553773689_04db512d Completed\n",
      "multi-class_classification_1553769673_d8124539 Completed\n",
      "multi-class_classification_1553768855_ca077e98 Completed\n",
      "multi-class_classification_1553703167_373b390a Completed\n",
      "multi-class_classification_1553702959_2bcfcfcc Completed\n",
      "multi-class_classification_1553701648_0e3944cd Completed\n",
      "multi-class_classification_1553699147_bb7ab0af Completed\n",
      "multi-class_classification_1553684399_b8d38d12 Failed\n",
      "multi-class_classification_1553684351_214ddcc5 Failed\n",
      "multi-class_classification_1553684328_5223fe93 Failed\n",
      "multi-class_classification_1553684301_0f1b7a07 Failed\n",
      "multi-class_classification_1553684247_631465da Failed\n",
      "multi-class_classification_1553680811_a63e3996 Completed\n",
      "multi-class_classification_1553680723_dc27a926 Completed\n",
      "multi-class_classification_1553680785_cb723fb8 Completed\n",
      "multi-class_classification_1553680703_ac9a6f7e Failed\n",
      "multi-class_classification_1553680678_d80d0e43 Failed\n",
      "multi-class_classification_1553680289_0a4f12c5 Failed\n",
      "multi-class_classification_1553680273_c1b6f338 Failed\n",
      "multi-class_classification_1553680244_2365b1e8 Failed\n",
      "multi-class_classification_1553680209_fa1c3109 Failed\n",
      "multi-class_classification_1553680184_cc35c502 Failed\n",
      "multi-class_classification_1553678566_241caf36 Canceled\n",
      "multi-class_classification_1553678543_e4b4daef Canceled\n",
      "multi-class_classification_1553678494_d88bfaef Canceled\n",
      "multi-class_classification_1553678455_470e86ca Canceled\n",
      "multi-class_classification_1553603541_d88b74cf Failed\n",
      "multi-class_classification_1553602280_03bd512c Failed\n",
      "multi-class_classification_1553601454_c1c49a29 Completed\n",
      "multi-class_classification_1553601202_38ded359 Failed\n",
      "multi-class_classification_1553600706_192504e9 Failed\n",
      "multi-class_classification_1553600357_edcbbcf4 Failed\n",
      "multi-class_classification_1552395260_ff63f595 Failed\n",
      "multi-class_classification_1552391966_9ecf5536 Failed\n",
      "multi-class_classification_1552390657_a670c821 Canceled\n",
      "multi-class_classification_1552234611_d13aa31f Completed\n",
      "multi-class_classification_1552234551_7c1e71ae Canceled\n",
      "multi-class_classification_1552234466_f3214ec7 Canceled\n",
      "multi-class_classification_1552038452_136e0004 Completed\n",
      "multi-class_classification_1552036395_7b0babc0 Failed\n",
      "multi-class_classification_1551993792_0043c68d Completed\n",
      "multi-class_classification_1551969804_85939a1c Completed\n",
      "multi-class_classification_1551962256_a1ed3440 Completed\n",
      "multi-class_classification_1551960911_4643c688 Failed\n",
      "multi-class_classification_1551887435_7124151d Failed\n",
      "multi-class_classification_1551886465_accda2c9 Failed\n",
      "multi-class_classification_1551885264_4194f72c Failed\n",
      "multi-class_classification_1551884797_0567f791 Failed\n",
      "multi-class_classification_1551884025_3c1ff795 Completed\n",
      "multi-class_classification_1551883969_fdf5defa Failed\n",
      "multi-class_classification_1551883826_304b1f1c Failed\n",
      "multi-class_classification_1551883612_e3e085d7 Completed\n",
      "multi-class_classification_1551883318_50e62188 Failed\n",
      "multi-class_classification_1551882973_6dee408f Failed\n",
      "multi-class_classification_1551882770_f76db873 Failed\n",
      "multi-class_classification_1551881518_a9151cc0 Failed\n",
      "multi-class_classification_1551881317_32dcae3d Failed\n",
      "multi-class_classification_1551881269_c1302161 Failed\n",
      "multi-class_classification_1551880945_de540ad8 Failed\n",
      "multi-class_classification_1551880448_86941086 Failed\n",
      "multi-class_classification_1551879274_0f876078 Failed\n",
      "multi-class_classification_1551879435_144caf8e Failed\n",
      "multi-class_classification_1551876853_03ef5c6b Failed\n",
      "multi-class_classification_1551872752_6b1a89a8 Failed\n",
      "multi-class_classification_1551803937_c7d9cf5f Failed\n",
      "multi-class_classification_1551801159_b8d14162 Failed\n",
      "multi-class_classification_1551799624_6dd5ca3d Failed\n",
      "multi-class_classification_1551799639_42f70fb0 Failed\n",
      "multi-class_classification_1551798408_6db7d8bd Failed\n",
      "multi-class_classification_1551798203_a7355bea Failed\n",
      "multi-class_classification_1551796860_b64b350c Failed\n",
      "multi-class_classification_1551358822_ea0feadc Failed\n",
      "multi-class_classification_1551312309_efbc9894 Failed\n",
      "multi-class_classification_1551312289_e8288cf8 Failed\n",
      "multi-class_classification_1551312260_973a285b Completed\n",
      "multi-class_classification_1551306089_efebafc8 Completed\n",
      "multi-class_classification_1551278785_10c7deeb Completed\n",
      "multi-class_classification_1551277843_5902f6cc Failed\n",
      "multi-class_classification_1551277818_db8ff661 Failed\n",
      "multi-class_classification_1551274041_6b2cab57 Failed\n",
      "multi-class_classification_1551268693_ad9243ca Failed\n",
      "multi-class_classification_1551259674_009fc1ff Failed\n",
      "multi-class_classification_1551197057_bf9371f6 Failed\n",
      "multi-class_classification_1551196530_93f4b7e9 Completed\n",
      "multi-class_classification_1551196174_507f0811 Failed\n",
      "multi-class_classification_1551193829_480640bd Failed\n",
      "multi-class_classification_1551188169_b8a42b31 Failed\n",
      "multi-class_classification_1551187122_3cc3e30e Failed\n",
      "multi-class_classification_1551186176_c2e8b5cc Failed\n",
      "multi-class_classification_1551182579_65e73c2a Failed\n",
      "multi-class_classification_1551179111_2341d45b Failed\n",
      "multi-class_classification_1553865385_99d5c49e Queued\n",
      "multi-class_classification_1551876359_660a7dd0 Failed\n",
      "multi-class_classification_1551876014_d5ee614d Failed\n",
      "multi-class_classification_1551875603_7f672612 Failed\n",
      "multi-class_classification_1551875121_8fbc6a46 Failed\n",
      "multi-class_classification_1551874568_62529c55 Failed\n",
      "multi-class_classification_1551873997_a7b9da22 Failed\n",
      "multi-class_classification_1551803129_6cdf5e0c Failed\n",
      "<class 'azureml.core.script_run.ScriptRun'> Failed\n"
     ]
    }
   ],
   "source": [
    "for r in exp.get_runs():  \n",
    "    print(r.id, r.get_status())\n",
    "    if r.get_status() not in ['Complete', 'Failed']:\n",
    "        r.cancel()\n",
    "\n",
    "# if you know the run id, you can \"rehydrate\" the run\n",
    "from azureml.core import get_run\n",
    "#r = get_run(experiment=exp, run_id=\"multi-class_classification_1552234551_7c1e71ae\", rehydrate=True)\n",
    "# check the returned run type and status\n",
    "print(type(r), r.get_status())\n",
    "\n",
    "# you can cancel a run if it hasn't completed or failed\n",
    "#if r.get_status() not in ['Complete', 'Failed']:\n",
    "    #r.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gray_img = rgb2gray(image)\n",
    "color = ('b','g','r')\n",
    "plt.figure()\n",
    "for i,col in enumerate(color):\n",
    "    histr = cv2.calcHist([image],[i],None,[256],[0,256])\n",
    "    plt.plot(histr,color = col)\n",
    "    plt.xlim([0,256])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()\n",
    "#print(x_train[0:9].values)\n",
    "#print(y_train[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(features[0][0])\n",
    "#print(features[1])\n",
    "'''\n",
    "fig, ax = plt.subplots(1, figsize=(8, 4))\n",
    "\n",
    "hog_image_rescaled = exposure.rescale_intensity(features[0][0][1], in_range=(0, 10))\n",
    "\n",
    "ax.axis('off')\n",
    "ax.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "ax.set_title('Histogram of Oriented Gradients')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "blobs_log = blob_log(gray_img, max_sigma=30, num_sigma=10, threshold=.1)\n",
    "\n",
    "blobs_log[:, 2] = blobs_log[:, 2] * sqrt(2)\n",
    "\n",
    "blobs_dog = blob_dog(gray_img, max_sigma=30, threshold=.1)\n",
    "blobs_dog[:, 2] = blobs_dog[:, 2] * sqrt(2)\n",
    "\n",
    "blobs_doh = blob_doh(gray_img, max_sigma=30, threshold=.01)\n",
    "\n",
    "blobs_list = [blobs_log, blobs_dog, blobs_doh]\n",
    "colors = ['yellow', 'lime', 'red']\n",
    "titles = ['Laplacian of Gaussian', 'Difference of Gaussian',\n",
    "          'Determinant of Hessian']\n",
    "sequence = zip(blobs_list, colors, titles)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(9, 3), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "for idx, (blobs, color, title) in enumerate(sequence):\n",
    "    ax[idx].set_title(title)\n",
    "    ax[idx].imshow(image, interpolation='nearest')\n",
    "    for blob in blobs:\n",
    "        y, x, r = blob\n",
    "        c = plt.Circle((x, y), r, color=color, linewidth=2, fill=False)\n",
    "        ax[idx].add_patch(c)\n",
    "    ax[idx].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fd, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 4))\n",
    "\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "ax.axis('off')\n",
    "ax.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "ax.set_title('Histogram of Oriented Gradients')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(gray_img)\n",
    "coords = corner_peaks(corner_harris(gray_img), min_distance=5)\n",
    "\n",
    "fig, (ax) = plt.subplots(1, figsize=(8, 4))\n",
    "\n",
    "ax.axis('off')\n",
    "ax.imshow(image)\n",
    "ax.plot(coords[:, 1], coords[:, 0], 'or', ms=4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "kp = sift.detect(gray,None)\n",
    "\n",
    "siftimg=cv2.drawKeypoints(gray,kp,img)\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(siftimg)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
